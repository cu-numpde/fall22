{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3471b94b",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2022-10-12 Krylov and preconditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159be0e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Last time\n",
    "\n",
    "* Classical iterative methods\n",
    "* Concept of preconditioning\n",
    "\n",
    "## Today\n",
    "* Krylov methods (focus on GMRES)\n",
    "* PETSc experiments\n",
    "* Simple preconditioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e82d841d",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "advdiff_matrix (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "default(linewidth=3)\n",
    "using LinearAlgebra\n",
    "using SparseArrays\n",
    "\n",
    "function my_spy(A)\n",
    "    cmax = norm(vec(A), Inf)\n",
    "    s = max(1, ceil(120 / size(A, 1)))\n",
    "    spy(A, marker=(:square, s), c=:diverging_rainbow_bgymr_45_85_c67_n256, clims=(-cmax, cmax))\n",
    "end\n",
    "\n",
    "function advdiff_matrix(n; kappa=1, wind=[0, 0])\n",
    "    h = 2 / (n + 1)\n",
    "    rows = Vector{Int64}()\n",
    "    cols = Vector{Int64}()\n",
    "    vals = Vector{Float64}()\n",
    "    idx((i, j),) = (i-1)*n + j\n",
    "    in_domain((i, j),) = 1 <= i <= n && 1 <= j <= n\n",
    "    stencil_advect = [-wind[1], -wind[2], 0, wind[1], wind[2]] / h\n",
    "    stencil_diffuse = [-1, -1, 4, -1, -1] * kappa / h^2\n",
    "    stencil = stencil_advect + stencil_diffuse\n",
    "    for i in 1:n\n",
    "        for j in 1:n\n",
    "            neighbors = [(i-1, j), (i, j-1), (i, j), (i+1, j), (i, j+1)]\n",
    "            mask = in_domain.(neighbors)\n",
    "            append!(rows, idx.(repeat([(i,j)], 5))[mask])\n",
    "            append!(cols, idx.(neighbors)[mask])\n",
    "            append!(vals, stencil[mask])\n",
    "        end\n",
    "    end\n",
    "    sparse(rows, cols, vals)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e5b17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Krylov subspaces\n",
    "\n",
    "All matrix iterations work with approximations in a *Krylov subspace*, which has the form\n",
    "\n",
    "$$ K_n = \\big[ b \\big| Ab \\big| A^2 b \\big| \\dotsm \\big| A^{n-1} b \\big] . $$\n",
    "\n",
    "This matrix is horribly ill-conditioned and cannot stably be computed as written.  Instead, we seek an orthogonal basis $Q_n$ that spans the same space as $K_n$.  We could write this as a factorization\n",
    "\n",
    "$$ K_n = Q_n R_n $$\n",
    "\n",
    "where the first column $q_0 = b / \\lVert b \\rVert$.  The $R_n$ is unnecessary and hopelessly ill-conditioned, so a slightly different procedure is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8701b02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Arnoldi iteration\n",
    "\n",
    "The Arnoldi iteration applies orthogonal similarity transformations to reduce $A$ to [Hessenberg form](https://en.wikipedia.org/wiki/Hessenberg_matrix), starting from a vector $q_0 = b$,\n",
    "\n",
    "$$ A = Q H Q^* . $$\n",
    "\n",
    "Let's multiply on the right by $Q$ and examine the first $n$ columns,\n",
    "\n",
    "$$ A Q_n = Q_{n+1} H_n $$\n",
    "where $H_n$ is an $(n+1) \\times n$ Hessenberg matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b3ce3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Conditioning of Arnoldi process\n",
    "The Arnoldi process is well-conditioned because $Q$ is orthogonal and\n",
    "\n",
    "$$ \\lVert H_n \\rVert \\le \\lVert Q_{n+1}^* \\rVert \\lVert A \\rVert \\lVert Q_n \\rVert \\le \\lVert A \\rVert $$.\n",
    "\n",
    "For a lower bound, we have\n",
    "\n",
    "$$ \\sigma_{\\min}(A)^2 \\le x^* A^* A x $$\n",
    "\n",
    "for all $x$ of norm 1.  It must also be true for any $x = Q_n y$ where $\\lVert y\\rVert = 1$, thus\n",
    "\n",
    "$$ \\sigma_{\\min}(A)^2 \\le y^* Q_n^* A^* A Q_n y = y^* H_n^* Q_{n+1}^* Q_{n+1} H_n y = y^* H_n^* H_n y . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63150de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GMRES\n",
    "$$ A Q_n = Q_{n+1} H_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e634ae5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "GMRES (Generalized Minimum Residual) minimizes\n",
    "$$ \\lVert A x - b \\rVert $$\n",
    "over the subspace $Q_n$.  I.e., $x = Q_n y$ for some $y$.  By the Arnoldi recurrence, this is equivalent to\n",
    "$$ \\lVert Q_{n+1} H_n y - b \\lVert $$\n",
    "which can be solved by minimizing\n",
    "$$ \\lVert H_n y - Q_{n+1}^* b \\rVert . $$\n",
    "Since $q_0 = b/\\lVert b \\lVert$, the least squares problem is to minimize\n",
    "$$ \\Big\\lVert H_n y - \\lVert b \\rVert e_0 \\Big\\rVert . $$\n",
    "The solution of this least squares problem is achieved by incrementally updating a $QR$ factorization of $H_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda12635",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Notes\n",
    "* The solution $x_n$ constructed by GMRES at iteration $n$ is not explicitly available.  If a solution is needed, it must be constructed by solving the $(n+1)\\times n$ least squares problem and forming the solution as a linear combination of the $n$ vectors $Q_n$.  The leading cost is $2mn$ where $m \\gg n$.\n",
    "* The residual vector $r_n = A x_n - b$ is not explicitly available in GMRES.  To compute it, first build the solution $x_n$ as above.\n",
    "* GMRES minimizes the 2-norm of the residual $\\lVert r_n \\rVert$ which is equivalent to the $A^* A$ norm of the error $\\lVert x_n - x_* \\rVert_{A^* A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f20ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More notes on GMRES\n",
    "\n",
    "* GMRES needs to store the full $Q_n$, which is unaffordable for large $n$ (many iterations).  The standard solution is to choose a \"restart\" $k$ and to discard $Q_n$ and start over with an initial guess $x_k$ after each $k$ iterations.  This algorithm is called GMRES(k).  PETSc's default solver is GMRES(30) and the restart can be controlled using the run-time option `-ksp_gmres_restart`.\n",
    "* Most implementations of GMRES use classical Gram-Schmidt because it is much faster in parallel (one reduction per iteration instead of $n$ reductions per iteration).  The PETSc option `-ksp_gmres_modifiedgramschmidt` can be used when you suspect that classical Gram-Schmidt may be causing instability.\n",
    "* There is a very similar (and older) algorithm called GCR that maintains $x_n$ and $r_n$.  This is useful, for example, if a convergence tolerance needs to inspect individual entries.  GCR requires $2n$ vectors instead of $n$ vectors, and can tolerate a nonlinear preconditioner.  FGMRES is a newer algorithm with similar properties to GCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ae2a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments in [PETSc](https://petsc.org)\n",
    "\n",
    "* PETSc = Portable Extensible Toolkit for Scientific computing\n",
    "* `./configure`, `make`\n",
    "  * Depends on BLAS and LAPACK (default system or package manager)\n",
    "    * or `./configure --download-f2cblaslapack --download-blis`\n",
    "  * Depends on MPI for parallelism (package manager)\n",
    "    * or `./configure --download-mpich` (or `--download-openmpi`)\n",
    "* `docker run -it --rm jedbrown/petsc`\n",
    "* Lots of examples (mostly C and Fortran, some Python)\n",
    "* Experimental bindings in Rust, Julia\n",
    "* We'll use `src/snes/tutorials/ex15.c`\n",
    "  * \"p-Bratu\": combines p-Laplacian with Bratu nonlinearity\n",
    "  $$ -\\nabla\\cdot\\big(\\kappa(\\nabla u) \\nabla u\\big) - \\lambda e^u = f $$\n",
    "  * Newton solver with Krylov on each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144760c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple preconditioners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c22dd4",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## Jacobi `-pc_type jacobi`\n",
    "\n",
    "$$ P_{\\text{Jacobi}}^{-1} = D^{-1} $$\n",
    "where $D$ is the diagonal of $A$.\n",
    "\n",
    "## Gauss-Seidel `-pc_type sor`\n",
    "\n",
    "$$ P_{GS}^{-1} = (L+D)^{-1} $$\n",
    "where $L$ is the (strictly) lower triangular part of $A$.  The upper triangular part may be used instead, or a symmetric form\n",
    "$$ P_{SGS}^{-1} = (L+U)^{-1} A \\Big( I - (L+D)^{-1} \\Big) . $$\n",
    "\n",
    "### Over-relaxation\n",
    "\n",
    "`-pc_sor_omega 1.` is default (Gauss-Seidel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7d362",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Run p-Laplacian example\n",
    "```console\n",
    "$ cd src/snes/tutorials\n",
    "$ make ex15\n",
    "$ ./ex15 -da_refine 2 -dm_view\n",
    "$ ./ex15 -ksp_monitor -pc_type jacobi\n",
    "$ ./ex15 -snes_view\n",
    "```\n",
    "\n",
    "## Experiments\n",
    "\n",
    "* How does iteration count vary under grid refinement?\n",
    "* How sensitive is it to parameters\n",
    "  * p-Laplacian `-p` $> 1$ and `-epsilon` $> 0$\n",
    "  * Bratu `-lambda` $< 6.8$\n",
    "* How sensitive to `-ksp_gmres_restart`?\n",
    "* `-ksp_monitor_true_residual`\n",
    "* `-ksp_view_eigenvalues`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359efcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symmetric problems\n",
    "\n",
    "## Lanczos iteration: like GMRES for symmetric problems\n",
    "\n",
    "If $A$ is symmetric, then $H = Q^T A Q$ is also symmetric.  Since $H$ is Hessenberg, this means $H$ is tridiagonal.  Instead of storing $Q_n$, it is sufficient to store only the last two columns since the iteration satisfies a 3-term recurrence.  The analog of GMRES for the symmetric case is called MINRES and is also useful for solving symmetric indefinite problems.\n",
    "\n",
    "## Conjugate Gradients: changing the norm\n",
    "\n",
    "Instead of minimizing the $A^T A$ norm of the error, the Conjugate Gradient method minimizes the $A$ norm of the error.  For $A$ to induce a norm, it must be symmetric positive definite.  [Jeremy Shewchuck's guide to CG](http://www.cs.cmu.edu/%7Equake-papers/painless-conjugate-gradient.pdf) is an excellent resource."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
